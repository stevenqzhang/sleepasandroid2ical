# -*- coding: utf-8 -*-# cleans sleep CSV FILES#Id,Tz,From,To,Sched,Hours,Rating,Comment,Framerate,Snore,Noise,Cycles,Eventfrom datetime import datetimefrom datetime import timedeltaimport urllib2import shutilimport urlparseimport osdef readSB(f):    sleeps = list()    keys = f.readline()    keys = keys.split(',')    #set to make sure we don't repeat stuff    unique_sleep_ids = set()    line_num = -1 # for debugging    while True:        line_num += 1        line = f.readline()        if not line: break        #data rows alwys start with a timestamp such as 1406218286546        if line.startswith('"1'):            values = line.split(',')        d = dict(zip(keys, values))        new_d = dict()        ## Data prep....        # processing certain keys        d["Comment"] = d["Comment"].strip('"').strip('#home').strip('#newmoon').strip('manually added')\            .strip("Nap length")        d["Rating"] = d["Rating"].strip('"')        d["StartDateTime"] = datetime.strptime(d['From'], '"%d. %m. %Y %H:%M"')        d["EndDateTime"] = datetime.strptime(d['To'], '"%d. %m. %Y %H:%M"')        # wrap beginning date        # only keep certain keys        for k in ["Id", "Tz", "StartDateTime", "EndDateTime", "Comment", "Hours"]:            new_d[k] = d[k]        # ignore duplicates        if (d["Id"] in unique_sleep_ids):            print "duplicate sleepcloud entry: " + d["Id"]        else:            sleeps.append(new_d)        unique_sleep_ids.add(d["Id"])    return sleeps'''given datetime object, and cutoff hour will return1) wrappedDate2) what else?'''def wrapDateTime(dt, dayEndHour = 20):    if dt.hour < dayEndHour:        return dt.date(), dt.hour + (dt.minute / 60.0)    else:        return dt.date() + timedelta(days=1), (dt.hour + (dt.minute / 60.0) - 24)def cleanCsv(sleeps, f):    lastDtendDay = -1    for event in cal.subcomponents:        start = event['dtstart'].dt        stop = event['dtend'].dt        #todo fix this boundary by actually grouping all times by days...        # define day boundary,        if(start.day != lastDtendDay):            f.write("================" +start.strftime("%m-%d-%Y")  + "=============<br/>")        lastDtendDay = stop.day        if "nap" in str(event['summary']):            pass;        # if sleep start is before midnight, and sleep end is after midnight, there should be ...        containsMidight = containsMidnight(start, stop)        if(containsMidight):            f.write("<font color='green'> GOOD! <br/> ")        f.write("sleep start" + start.strftime('%X') + "<br/> sleep end:" + stop.strftime('%X') + "<br/><br/>")        #print ("sleep start" + start.strftime('%X') + "<br/> sleep end:" + stop.strftime('%X') + "<br/><br/>")        #print start.strftime('%x')        print "wrote record starting at " + start.strftime('%X')        if(containsMidight):            f.write("</font>")    print "Finished writing HTML"    f.close()# from http://stackoverflow.com/a/2067142/1621636def download(url, fileName=None):    def getFileName(url,openUrl):        if 'Content-Disposition' in openUrl.info():            # If the response has Content-Disposition, try to get filename from it            cd = dict(map(                lambda x: x.strip().split('=') if '=' in x else (x.strip(),''),                openUrl.info()['Content-Disposition'].split(';')))            if 'filename' in cd:                filename = cd['filename'].strip("\"'")                if filename: return filename        # if no filename was found above, parse it out of the final URL.        return os.path.basename(urlparse.urlsplit(openUrl.url)[2])    r = urllib2.urlopen(urllib2.Request(url))    try:        fileName = fileName or getFileName(url,r)        with open(fileName, 'wb') as f:            shutil.copyfileobj(r,f)    finally:        r.close()def test():    csvfile = open("/Users/steven/dev/sleep2ical/test100rows.txt", 'rb')    readSB(csvfile)if __name__ == "__main__":    test()    #main()def main():    import sys    import argparse    parser = argparse.ArgumentParser()    parser.add_argument('--cleancomments', required=false)    #html, ical, or csv    parser.add_argument('--outtype', required=true)    parser.add_argument('--csv_file', required=true)    args = parser.parse_args()    download(sys.argv[1], "temp_csv.txt")    csvfile = open("temp_csv.txt", 'rb')    sleeps = readSB(csvfile)    icalfile = open(sys.argv[2], 'wb')    cleanFlag = False    if sys.argv[3] == "-c":        cleanFlag = True    #todo get these to parse with blanks    htmlFlag = False    if sys.argv[4] == "-p":        htmlFlag = True    cleanCsv(sleeps, icalfile)