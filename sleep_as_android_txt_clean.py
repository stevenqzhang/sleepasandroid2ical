# -*- coding: utf-8 -*-# cleans sleep CSV FILES#Id,Tz,From,To,Sched,Hours,Rating,Comment,Framerate,Snore,Noise,Cycles,Eventfrom datetime import datetimefrom datetime import timedeltaimport urllib2import shutilimport urlparseimport osdef readSB(f):    sleeps = list()    keys = f.readline()    keys = keys.split(',')    #set to make sure we don't repeat stuff    unique_sleep_ids = set()    line_num = -1 # for debugging    while True:        line_num += 1        line = f.readline()        if not line: break        #data rows alwys start with a timestamp such as 1406218286546        if line.startswith('"1'):            values = line.split(',')        d = dict(zip(keys, values))        new_d = dict()        ## Data prep....        # processing certain keys        d["Hours"] = d["Hours"].strip('"')        d["Comment"] = d["Comment"].strip('"').strip('#home').strip('#newmoon').strip('manually added')\            .strip("Nap length")        d["Rating"] = d["Rating"].strip('"')        d["StartDateTime"] = datetime.strptime(d['From'], '"%d. %m. %Y %H:%M"')        d["EndDateTime"] = datetime.strptime(d['To'], '"%d. %m. %Y %H:%M"')        # wrap beginning date        d["startDateWrapped"], d["startHourWrapped"] = wrapDateTime(d["StartDateTime"])        d["endDateWrapped"], d["endHourWrapped"] = wrapDateTime(d["EndDateTime"])        #debug: double check        ## print d["Hours"], d["startHourWrapped"] - d["endHourWrapped"]        # only keep certain keys        for k in ["Id", "Tz", "StartDateTime", "EndDateTime",                  "startDateWrapped", "startHourWrapped",                  "endDateWrapped", "endHourWrapped",                  "Comment", "Hours"]:            new_d[k] = d[k]        # ignore duplicates        if (d["Id"] in unique_sleep_ids):            print "duplicate sleepcloud entry: " + d["Id"]        # ignore napping and all of that        elif ("#nap" in d["Comment"]):            print "ignoring naps" + d["Id"]        elif ("#failedToSleep" in d["Comment"]):            print "ignoring failedtosleep" + d["Id"]        else:            sleeps.append(new_d)        unique_sleep_ids.add(d["Id"])    return sleeps'''given datetime object, and cutoff hour will return'''def wrapDateTime(dt, dayEndHour = 20):    if dt.hour < dayEndHour:        return dt.date(), dt.hour + (dt.minute / 60.0)    else:        return dt.date() + timedelta(days=1), (dt.hour + (dt.minute / 60.0) - 24)def analyze(sleeps):    # make a map from day to sleeps    startDate_to_sleeps_map = dict()    for sleep in sleeps:        startDate_to_sleeps_map.setdefault(sleep["startDateWrapped"], []).append(sleep)    # rank and sort each guy    new_startDate_to_sleeps_map = dict()    sorted_sleeps = []    #result container    allSleepsFlattened = []    for day, sleeps in startDate_to_sleeps_map.items():        sorted_sleeps = sorted(sleeps, key= lambda k:k["startHourWrapped"])        #todo do this for all categories        totalSleepForTheDay = 0        for idx, sleep in enumerate(sorted_sleeps):            try:                totalSleepForTheDay += float(sleep["Hours"])            except:                print sleep["Hours"]        for sleep in sorted_sleeps:            sleep["totalHoursForDay"] = totalSleepForTheDay            sleep["index"] = idx            sleep["numberOfSegmentsForDay"] = len(sorted_sleeps)        allSleepsFlattened.extend(sorted_sleeps)    # write to file    import csv    keys = allSleepsFlattened[0].keys()    with open('/Users/steven/Dropbox/1_current/concussion/wellness dashboard/data/sleep_analyzed.csv','wb') as f:        dict_writer = csv.DictWriter(f, keys)        dict_writer.writeheader()        dict_writer.writerows(allSleepsFlattened)def test():    csvfile = open("/Users/steven/Dropbox/1_current/concussion/wellness dashboard/data/sleep_as_android_autocopied_from_dropbox.txt", 'rb')    analyze(readSB(csvfile))if __name__ == "__main__":    test()    #main()# todo add a main